# ğŸ“š Bookwise Backend

Backend oficial para la plataforma **Bookwise**, un sistema inteligente de recomendaciÃ³n y disponibilidad de libros en bibliotecas pÃºblicas de Chile.

## ğŸš€ Arquitectura

El sistema utiliza una arquitectura moderna basada en **Node.js** y **Python**, desacoplando la lÃ³gica de negocio de la recolecciÃ³n de datos (scraping).

### Stack TecnolÃ³gico
- **Core API**: Node.js + Express (Puerto 3001)
- **Base de Datos**: PostgreSQL (vÃ­a Supabase Connection Pooler)
- **ORM**: Sequelize
- **IA**: Cohere AI (GeneraciÃ³n de recomendaciones)
- **Scraping**: Python 3.x (Master/Worker Pattern)

---

## ğŸ—ï¸ Estructura del Proyecto

```
backend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/         # ConfiguraciÃ³n de BD (Sequelize)
â”‚   â”œâ”€â”€ controllers/    # LÃ³gica de endpoints (Books, Recommendations)
â”‚   â”œâ”€â”€ models/         # Modelos de datos (Book.js)
â”‚   â”œâ”€â”€ routes/         # DefiniciÃ³n de rutas API
â”‚   â””â”€â”€ services/       # Servicios externos (Cohere, Cron Manager)
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ bibliometro_urls.py    # [Master] Recolector de URLs
â”‚   â”œâ”€â”€ bibliometro_details.py # [Worker] Extractor de detalles
â”‚   â””â”€â”€ requirements.txt       # Dependencias de Python
â””â”€â”€ server.js           # Punto de entrada
```

---

## âš™ï¸ ConfiguraciÃ³n e InstalaciÃ³n

### 1. Requisitos Previos
- Node.js v18+
- Python 3.10+
- PostgreSQL (Supabase)

### 2. Variables de Entorno (`.env`)
Crear un archivo `.env` en la raÃ­z con:

```env
# Servidor
PORT=3001
API_SECRET=tu_secreto_para_scrapers

# Base de Datos (Supabase Transaction Pooler)
DATABASE_URL=postgresql://user:pass@aws-0-us-east-1.pooler.supabase.com:6543/postgres

# IA Provider
COHERE_API_KEY=tu_api_key_cohere
```

### 3. InstalaciÃ³n de Dependencias

**Node.js (Backend):**
```bash
npm install
```

**Python (Scrapers):**
Se recomienda crear un entorno virtual:
```bash
python -m venv .venv
source .venv/bin/activate  # Mac/Linux
.\.venv\Scripts\Activate   # Windows
pip install -r scrapers/requirements.txt
```

---

## ğŸ•·ï¸ Sistema de Scraping (Dos Fases)

Para evitar bloqueos y optimizar recursos, el scraping se divide en dos procesos secuenciales gestionados por **Cron Jobs**:

1.  **Fase 1: Master (`bibliometro_urls.py`)** - *03:00 AM*
    *   Escanea sitemaps y categorÃ­as de Bibliometro.
    *   Genera un archivo `bibliometro_final_urls.txt` con todos los enlaces a libros.
    *   *No conecta a la BD.*

2.  **Fase 2: Worker (`bibliometro_details.py`)** - *04:00 AM*
    *   Lee el archivo de texto generado.
    *   Visita cada link para extraer: TÃ­tulo, Autor, Portada y **Disponibilidad por Sucursal**.
    *   EnvÃ­a los datos a la API (`POST /api/books/batch`) usando el `API_SECRET`.

---

## ğŸ“¡ API Endpoints Principales

| MÃ©todo | Endpoint | DescripciÃ³n |
| :--- | :--- | :--- |
| `GET` | `/api/books` | Lista libros paginados. |
| `GET` | `/api/books/search` | BÃºsqueda por tÃ­tulo o autor. |
| `POST` | `/api/recommendations` | Genera recomendaciÃ³n con IA. |
| `POST` | `/api/books/batch` | **(Interno)** Carga masiva de libros desde scrapers. |

---

## ğŸ§ª Comandos Ãštiles

```bash
# Iniciar servidor en desarrollo
npm run dev

# Ejecutar scraper manualmente (Fase 1)
python scrapers/bibliometro_urls.py

# Ejecutar scraper manualmente (Fase 2)
python scrapers/bibliometro_details.py
```
